{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Click Stream Data with the GraphX Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxEdgesFirstShell = 25\n",
       "maxEdgesNthShell = 20\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxEdgesFirstShell = 25\n",
    "val maxEdgesNthShell = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a look at the Wikipedia ClickStream Dataset (TODO:  Link to original dataset).  This dataset reports thee number of times a user goes from one Wikipedia site to another.  The first two columns are the page numbers of the \"from\" site and \"to\" site, respectively (as indexed numbers).  The third column is the the number of clicks (the edge values).  The remaining columns are the \"from\" and \"to\" sites (as names). If a \"from\" site is outside of the Wikipedia corpus, it is listed as \"other\".  We will remove these sites, because it is not interesting to count those entries, and the numbers are much larger than the clickstream traffic within Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3632887\t93\tother-wikipedia\t!!\tother\n",
      "\t3632887\t46\tother-empty\t!!\tother\n",
      "\t3632887\t10\tother-other\t!!\tother\n",
      "64486\t3632887\t11\t!_(disambiguation)\t!!\tother\n",
      "2061699\t2556962\t19\tLouden_Up_Now\t!!!_(album)\tlink\n",
      "\t2556962\t25\tother-empty\t!!!_(album)\tother\n",
      "\t2556962\t16\tother-google\t!!!_(album)\tother\n",
      "\t2556962\t44\tother-wikipedia\t!!!_(album)\tother\n",
      "64486\t2556962\t15\t!_(disambiguation)\t!!!_(album)\tlink\n",
      "600744\t2556962\t297\t!!!\t!!!_(album)\tlink\n",
      "\t6893310\t11\tother-empty\t!Hero_(album)\tother\n",
      "1921683\t6893310\t26\t!Hero\t!Hero_(album)\tlink\n",
      "\t6893310\t16\tother-wikipedia\t!Hero_(album)\tother\n",
      "\t6893310\t23\tother-google\t!Hero_(album)\tother\n",
      "8127304\t22602473\t16\tJericho_Rosales\t!Oka_Tokat\tlink\n",
      "35978874\t22602473\t20\tList_of_telenovelas_of_ABS-CBN\t!Oka_Tokat\tlink\n",
      "\t22602473\t57\tother-google\t!Oka_Tokat\tother\n",
      "\t22602473\t12\tother-wikipedia\t!Oka_Tokat\tother\n",
      "7360687\t22602473\t10\tRica_Peralejo\t!Oka_Tokat\tlink\n",
      "37104582\t22602473\t11\tJeepney_TV\t!Oka_Tokat\tlink\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines = headPlusWatsonPlusTeslaPlusApple.tsv MapPartitionsRDD[3940] at textFile at <console>:58\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "headPlusWatsonPlusTeslaPlusApple.tsv MapPartitionsRDD[3940] at textFile at <console>:58"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"headPlusWatsonPlusTeslaPlusApple.tsv\")\n",
    "lines.take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Some ETL Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some ETL that we will need to carry out in order to get a graph model that is worth using.  One such function is a simple indexing of the \"other\" sites so that we can filter them out later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getIndex: (x: String)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// simple mapping of 'other names to a unique index'\n",
    "def getIndex(x: String): Int = x match {\n",
    "    case \"other-wikipedia\" => 1\n",
    "    case \"other-empty\"     => 2\n",
    "    case \"other-internal\"  => 3    \n",
    "    case \"other-google\"    => 4\n",
    "    case \"other-yahoo\"     => 5\n",
    "    case \"other-bing\"      => 6\n",
    "    case \"other-facebook\"  => 7\n",
    "    case \"other-twitter\"   => 8 \n",
    "    case \"other-other\"     => 9 \n",
    "    case _ => 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more ETL Functions here.  We need to populate empty fields with null values that have the same type.  The API here is a bit less robust than the DataFrames API, but nothing we can't handle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignOtherSourceIndex: (partsLine: Array[String])String\n",
       "fillWithNum: (element: String)String\n",
       "fillWithString: (element: String, colNum: Int)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def assignOtherSourceIndex(partsLine:Array[String]): String = {\n",
    "    println( \"assigning other source to \" + partsLine(0))\n",
    "    if (partsLine(0) == \"\") {\n",
    "//      println(\"empty\")\n",
    "      return getIndex(partsLine(3)).toString\n",
    "    }\n",
    "    else {\n",
    "//       println(\"in nonblank\")\n",
    "       return partsLine(0)\n",
    "    }\n",
    "}\n",
    "// filling empty number fields with a large number\n",
    "def fillWithNum(element: String): String = {\n",
    "    if (element == \"\"){ 999999999.toString }\n",
    "    else {element}\n",
    "}\n",
    "\n",
    "// filling empty column fields with a filler string\n",
    "def fillWithString(element: String, colNum: Int): String = {\n",
    "    if (element == \"\"){\"column\"+colNum+\"Fill\"}\n",
    "    else{element}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to take our raw RDD and add all of our ETL functions.  We're filtering out redlinks, as well as all clicktstreams from outside Wikipedia, and also handling empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:345)\n",
       "  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:371)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:370)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.map(RDD.scala:370)\n",
       "  ... 60 elided\n",
       "Caused by: java.io.NotSerializableException: java.io.PrintWriter\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: java.io.PrintWriter, value: java.io.PrintWriter@77453e94)\n",
       "\t- field (class: $iw, name: pw, type: class java.io.PrintWriter)\n",
       "\t- object (class $iw, $iw@1039745f)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4f809e21)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5bde6ffd)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@898e143)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@57681b0e)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5e45ac8c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5825ded2)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7afc6c45)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@3776fea6)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@27bdb1a1)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@44619529)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@6b46a3c0)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1dd671a9)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4b16517c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4f916a39)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7a6d436e)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@38156618)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@2be65bcc)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@85d9e6a)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@24069bda)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@79d07eab)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@45e2e445)\n",
       "\t- field (class: $line167.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line167.$read, $line167.$read@65b1237)\n",
       "\t- field (class: $iw, name: $line167$read, type: class $line167.$read)\n",
       "\t- object (class $iw, $iw@78c4060b)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5a41a0d0)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7655246c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@de2c1fa)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@ee8d5cd)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@2f8fb03)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@15a7c2c6)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7d13229c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7199ab8c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@64f48b90)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@eb672c2)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@17c4005b)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@1648c216)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@292acd1c)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@70d960c5)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@56f96ea3)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@5862a16b)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@33178303)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@36c3bb82)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@f75da11)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@4fc165ac)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@7ee3e3f0)\n",
       "\t- field (class: $iw, name: $iw, type: class $iw)\n",
       "\t- object (class $iw, $iw@39b7a11)\n",
       "\t- field (class: $line170.$read, name: $iw, type: class $iw)\n",
       "\t- object (class $line170.$read, $line170.$read@5d4a2eb4)\n",
       "\t- field (class: $iw, name: $line170$read, type: class $line170.$read)\n",
       "\t- object (class $iw, $iw@54ee59d3)\n",
       "\t- field (class: $iw, name: $outer, type: class $iw)\n",
       "\t- object (class $iw, $iw@78184312)\n",
       "\t- field (class: $anonfun$3, name: $outer, type: class $iw)\n",
       "\t- object (class $anonfun$3, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parts = lines.map(l=>l.split(\"\\t\")).filter(l => !(l.contains(\"redlink\"))). //splitting on tabs and filtering out redlinks\n",
    "                                                    map(l => Array(assignOtherSourceIndex(l), \n",
    "                                                    fillWithNum(l(1)),fillWithNum(l(2)),\n",
    "                                                    fillWithString(l(3),4),fillWithString(l(4),5))).\n",
    "                                                    filter(y=>y(0).toInt>9)\n",
    "\n",
    "parts.take(20).foreach(x => println(x(0)+\"\\t\" + x(1) + \"\\t\" + x(2) + \"\\t\" + x(3) + \"\\t\" + x(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this into a DataFrame.  For this notebook, the DataFrame is only used briefly to show the column labels.  The `parts` RDD is used to construct the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Fields\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Fields(prev_id: Int, curr_id: Int, n: Int, \n",
    "           prev_title: String, curr_title: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+--------------------+----------------+\n",
      "| prev_id| curr_id|  n|          prev_title|      curr_title|\n",
      "+--------+--------+---+--------------------+----------------+\n",
      "|   64486| 3632887| 11|  !_(disambiguation)|              !!|\n",
      "| 2061699| 2556962| 19|       Louden_Up_Now|     !!!_(album)|\n",
      "|   64486| 2556962| 15|  !_(disambiguation)|     !!!_(album)|\n",
      "|  600744| 2556962|297|                 !!!|     !!!_(album)|\n",
      "| 1921683| 6893310| 26|               !Hero|   !Hero_(album)|\n",
      "| 8127304|22602473| 16|     Jericho_Rosales|      !Oka_Tokat|\n",
      "|35978874|22602473| 20|List_of_telenovel...|      !Oka_Tokat|\n",
      "| 7360687|22602473| 10|       Rica_Peralejo|      !Oka_Tokat|\n",
      "|37104582|22602473| 11|          Jeepney_TV|      !Oka_Tokat|\n",
      "|34376590|22602473| 22|Oka_Tokat_(2012_T...|      !Oka_Tokat|\n",
      "|31976181| 6810768| 51|List_of_death_met...|      !T.O.O.H.!|\n",
      "| 1337475| 3243047|208|The_Dismemberment...|       !_(album)|\n",
      "| 3284285| 3243047| 78|The_Dismemberment...|       !_(album)|\n",
      "| 2098292|  899480| 58|United_States_mil...|      \"A\"_Device|\n",
      "|  194844|  899480| 15| USS_Yorktown_(CV-5)|      \"A\"_Device|\n",
      "|  878246|  899480| 11|American_Defense_...|      \"A\"_Device|\n",
      "|  855901|  899480| 24|Overseas_Service_...|      \"A\"_Device|\n",
      "|  206427|  899480| 33|   USS_Ranger_(CV-4)|      \"A\"_Device|\n",
      "|  773691|  899480| 47|Antarctica_Servic...|      \"A\"_Device|\n",
      "| 2301720| 1282996| 43|     Kinsey_Millhone|\"A\"_Is_for_Alibi|\n",
      "+--------+--------+---+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "clicksDataFrame = [prev_id: int, curr_id: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[prev_id: int, curr_id: int ... 3 more fields]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clicksDataFrame = parts.map(\n",
    "     p => Fields(p(0).toInt, p(1).toInt, p(2).toInt, p(3), p(4))).toDF\n",
    "\n",
    "// registering dataframe\n",
    "clicksDataFrame.registerTempTable(\"clicks\")\n",
    "clicksDataFrame.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to explicitly deduplicate our vertices before building the graph.  The graphX API will handle this for us, but It saves us some time later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nodes1 = MapPartitionsRDD[3948] at map at <console>:71\n",
       "uniqueNodes1 = MapPartitionsRDD[3953] at map at <console>:74\n",
       "nodes2 = MapPartitionsRDD[3954] at map at <console>:76\n",
       "uniqueNodes2 = MapPartitionsRDD[3959] at map at <console>:79\n",
       "uniqueNodesBoth = MapPartitionsRDD[3966] at map at <console>:87\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8204"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//nodes should be deduplicated\n",
    "val nodes1 = (parts.map{p => Array(p(0).toString, p(3)) })\n",
    "nodes1.cache()\n",
    "val uniqueNodes1 = nodes1.map(x => x(0)+\"-0-\"+x(1)).distinct.  //trick for accessing distinct\n",
    "    map(x => Array(x.split(\"-0-\")(0), x.split(\"-0-\")(1)))  //resplitting to original structure\n",
    "\n",
    "val nodes2 = (parts.map{p => Array(p(1).toString, p(4)) })\n",
    "nodes2.cache()             \n",
    "val uniqueNodes2 = nodes2.map(x => x(0)+\"-0-\"+x(1)).distinct.  //trick for accessing distinct\n",
    "    map(x => Array(x.split(\"-0-\")(0), x.split(\"-0-\")(1)))  //resplitting to original structure\n",
    "                         //converting to vertex RDD\n",
    "\n",
    "\n",
    "uniqueNodes1.cache\n",
    "uniqueNodes2.cache\n",
    "val uniqueNodesBoth = (uniqueNodes1 ++ uniqueNodes2).map(x => x(0)+\"-0-\"+x(1)).distinct.  //trick for accessing distinct\n",
    "    map(x=>Array(x.split(\"-0-\")(0), x.split(\"-0-\")(1))).  //resplitting to original structure\n",
    "    map{ x=> (x(0).toInt.toLong, (x(1), x(1))) }  \n",
    "uniqueNodesBoth.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we construct the graph.  A graph consists of an RDD of vertices, and an RDD of edges, along with some error handling for dangling edges.  A printout of the first 10 edges and vertices is listed below.  Note that the vertices have some extra label annotation, while the edges have only the integer indexing and the edge value.  This is because the edges RDD is much larger than the vertices RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "first 10 vertices\n",
      "(150824,(Maiden,_North_Carolina,Maiden,_North_Carolina))\n",
      "(148720,(Quiet_Riot,Quiet_Riot))\n",
      "(612052,(Spider-Man_2,Spider-Man_2))\n",
      "(24319476,(Billy_Horschel,Billy_Horschel))\n",
      "(56892,(Classic_Environment,Classic_Environment))\n",
      "(32300828,(Ultimate_Fallout,Ultimate_Fallout))\n",
      "(1406584,(CCR_and_CAR_algebras,CCR_and_CAR_algebras))\n",
      "(24752844,(Français_Pour_une_Nuit,Français_Pour_une_Nuit))\n",
      "(1647936,(Luria–Delbrück_experiment,Luria–Delbrück_experiment))\n",
      "(1140076,(1982_Formula_One_season,1982_Formula_One_season))\n",
      "\n",
      "first 10 edges\n",
      "Edge(878,1424517,20)\n",
      "Edge(1130,1766908,14)\n",
      "Edge(1162,18938265,33)\n",
      "Edge(1348,821939,28)\n",
      "Edge(1495,155375,79)\n",
      "Edge(1869,175149,308)\n",
      "Edge(2678,2676,11)\n",
      "Edge(2724,4864529,148)\n",
      "Edge(2824,2001051,25)\n",
      "Edge(3382,3411,4720)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class nodeFields\n",
       "edges = MapPartitionsRDD[3967] at map at <console>:39\n",
       "defaultNode = (default Node,Missing)\n",
       "graph = org.apache.spark.graphx.impl.GraphImpl@6cf3ae72\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@6cf3ae72"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// importing the graphx library\n",
    "import org.apache.spark.graphx._\n",
    "case class nodeFields(nodeID: Int, nodeName: String)\n",
    " \n",
    "val edges = parts.map(x => Edge(x(0).toInt.toLong,x(1).toInt.toLong, x(2)) )\n",
    "val defaultNode = (\"default Node\", \"Missing\")\n",
    "\n",
    "//Graph is a wrapper to a vertex list and a edge list, with the defaultNode as well.\n",
    "//It does some internal bookkeepping to maintain consistency before packaging.\n",
    "val graph = Graph(uniqueNodesBoth,edges,defaultNode)\n",
    "//main graph will be searched very frequently\n",
    "graph.cache()\n",
    "\n",
    "println(\"\\nfirst 10 vertices\")\n",
    "graph.vertices.take(10).foreach(println)\n",
    "\n",
    "println(\"\\nfirst 10 edges\")\n",
    "graph.edges.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the top N edges connected to any particular node and discard the remaining edges.  This is known as graph pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pruneGraphByMaxEdges: (maxEdges: Int, bigGraph: org.apache.spark.graphx.Graph[(String, String),String])org.apache.spark.graphx.Graph[(String, String),String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pruneGraphByMaxEdges(maxEdges: Int,  bigGraph:  Graph[(String, String),String]): \n",
    "                                                    Graph[(String, String),String] = {\n",
    "    val minCount = bigGraph.triplets.sortBy(_.attr.toInt, ascending=false).\n",
    "                                    map(x=>x.attr.toInt).take(maxEdges).reverse(0)\n",
    "\n",
    "    return bigGraph.subgraph(epred = x => x.attr.toInt >= minCount)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our pruning function in hand, we want to start with a particular node and build a *shell* of N nodes around that central node.   ONce that graph is built, we want to add a shell to each node of that graph.  We can do this as many times as we like, but we're only creating 3 shells in this notebook.  Ultimately, this will give us a list of all important wikipedia sites that are 4 clicks or less away from the source site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addShellToGraph: (thisGraph: org.apache.spark.graphx.Graph[(String, String),String], shellNum: Int)org.apache.spark.graphx.Graph[(String, String),String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def addShellToGraph(thisGraph: Graph[(String, String),String], shellNum:Int ): Graph[(String, String),String]  = {\n",
    "\n",
    "    val searchStringList = thisGraph.triplets.map(x => x.srcAttr._1).collect\n",
    "    def recursiveGraphBuild(i:Int,prevGraph: Graph[(String, String),String] ):Graph[(String, String),String] = {\n",
    "        \n",
    "        val currentGraph =  pruneGraphByMaxEdges(maxEdgesNthShell,\n",
    "                            graph.subgraph(epred = x => x.srcAttr._1 == searchStringList(i))\n",
    "                            )\n",
    "\n",
    "        val nextGraph = Graph( graph.vertices, \n",
    "                                (prevGraph.edges++currentGraph.edges).distinct     )\n",
    "\n",
    "        if (i == searchStringList.length - 1 ) {   \n",
    "            return nextGraph\n",
    "        }\n",
    "        else {return recursiveGraphBuild(i+1, nextGraph)}\n",
    "    }\n",
    "    var i = 0\n",
    "    return recursiveGraphBuild(0,thisGraph)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's build our clickstream graph!  We'll start with the Watson site.  Some other sites are listed there as well.  Feel free to return to this and generate graphs with those after runnig through the first example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "centerVertex = Watson_(computer)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Watson_(computer)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// MAIN QUERY  ============================================\n",
    "//Here are a list of sites that work well for the prototype dataset\n",
    "\n",
    "val centerVertex = \"Watson_(computer)\"\n",
    "//val centerVertex = \"Heroes\"\n",
    "//val centerVertex = \"Tesla_Motors\"\n",
    "//val centerVertex = \"Apple_Inc.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the first cell of sites around the center vertex (Watson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smallGraph = org.apache.spark.graphx.impl.GraphImpl@6357d812\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@6357d812"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val smallGraph = pruneGraphByMaxEdges(maxEdgesFirstShell, \n",
    "                 graph.subgraph(epred = x => x.dstAttr._1.contains(centerVertex) && \n",
    "                               !(x.srcAttr._1.contains(\"other-\")) && \n",
    "                               !(x.srcAttr._1 == \"Main_Page\"))\n",
    "                )\n",
    "smallGraph.cache\n",
    "//smallGraph.triplets.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of *all* of the vertices connected to Watson.  We'll build our larger graph from this one.  We're calling the `triplets` method, which returns the edge between two vertices (one is always Watson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1164,(Artificial_intelligence,Artificial_intelligence)),(22584291,(Watson_(computer),Watson_(computer))),117)\n",
      "((2142,(List_of_artificial_intelligence_projects,List_of_artificial_intelligence_projects)),(22584291,(Watson_(computer),Watson_(computer))),119)\n",
      "((23485,(Prolog,Prolog)),(22584291,(Watson_(computer),Watson_(computer))),99)\n",
      "((30657,(Terabyte,Terabyte)),(22584291,(Watson_(computer),Watson_(computer))),178)\n",
      "((49387,(Deep_Blue_(chess_computer),Deep_Blue_(chess_computer))),(22584291,(Watson_(computer),Watson_(computer))),259)\n",
      "((136764,(Blue_Gene,Blue_Gene)),(22584291,(Watson_(computer),Watson_(computer))),113)\n",
      "((753973,(Ken_Jennings,Ken_Jennings)),(22584291,(Watson_(computer),Watson_(computer))),897)\n",
      "((886996,(Watson,Watson)),(22584291,(Watson_(computer),Watson_(computer))),412)\n",
      "((1400125,(Thomas_J._Watson_Research_Center,Thomas_J._Watson_Research_Center)),(22584291,(Watson_(computer),Watson_(computer))),53)\n",
      "((1813537,(POWER7,POWER7)),(22584291,(Watson_(computer),Watson_(computer))),36)\n",
      "((1937560,(Brad_Rutter,Brad_Rutter)),(22584291,(Watson_(computer),Watson_(computer))),247)\n",
      "((2894560,(History_of_artificial_intelligence,History_of_artificial_intelligence)),(22584291,(Watson_(computer),Watson_(computer))),41)\n",
      "((7283182,(History_of_IBM,History_of_IBM)),(22584291,(Watson_(computer),Watson_(computer))),59)\n",
      "((21903944,(Wolfram_Alpha,Wolfram_Alpha)),(22584291,(Watson_(computer),Watson_(computer))),140)\n",
      "((25671650,(Computer_performance_by_orders_of_magnitude,Computer_performance_by_orders_of_magnitude)),(22584291,(Watson_(computer),Watson_(computer))),31)\n",
      "((27748226,(Jeopardy!,Jeopardy!)),(22584291,(Watson_(computer),Watson_(computer))),38)\n",
      "((29339253,(List_of_Jeopardy!_tournaments_and_events,List_of_Jeopardy!_tournaments_and_events)),(22584291,(Watson_(computer),Watson_(computer))),27)\n",
      "((30816182,(Timeline_of_electrical_and_electronic_engineering,Timeline_of_electrical_and_electronic_engineering)),(22584291,(Watson_(computer),Watson_(computer))),30)\n",
      "((31512491,(Wii_U,Wii_U)),(22584291,(Watson_(computer),Watson_(computer))),44)\n",
      "((32472154,(Deep_learning,Deep_learning)),(22584291,(Watson_(computer),Watson_(computer))),181)\n",
      "((33530382,(Ginni_Rometty,Ginni_Rometty)),(22584291,(Watson_(computer),Watson_(computer))),69)\n",
      "((37764426,(Outline_of_natural_language_processing,Outline_of_natural_language_processing)),(22584291,(Watson_(computer),Watson_(computer))),48)\n",
      "((40379651,(IBM,IBM)),(22584291,(Watson_(computer),Watson_(computer))),1173)\n",
      "((41755648,(Google_DeepMind,Google_DeepMind)),(22584291,(Watson_(computer),Watson_(computer))),74)\n",
      "((43638412,(Humans_Need_Not_Apply,Humans_Need_Not_Apply)),(22584291,(Watson_(computer),Watson_(computer))),30)\n"
     ]
    }
   ],
   "source": [
    "smallGraph.triplets.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build out the additional shells from this original graph.  We're using a very small dataset here, so it only takes about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxEdgesPerFirstShell = 50\n",
       "maxEdgesPerNthShell = 20\n",
       "t0 = 135585393432676\n",
       "graph1p1 = org.apache.spark.graphx.impl.GraphImpl@7013683b\n",
       "graph2p1 = org.apache.spark.graphx.impl.GraphImpl@42f6ccbc\n",
       "graph3p1 = org.apache.spark.graphx.impl.GraphImpl@46a6d121\n",
       "dt = 32.389\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "32.389"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//smallGraph.triplets.count\n",
    "val maxEdgesPerFirstShell = 50\n",
    "val maxEdgesPerNthShell = 20\n",
    "\n",
    "val t0 = System.nanoTime\n",
    "// called 3 times (manually)\n",
    "val graph1p1 = addShellToGraph(smallGraph,2)\n",
    "val graph2p1 = addShellToGraph(graph1p1,3)\n",
    "// fourth click takes longer and is not very informative (so far)\n",
    "val graph3p1 = addShellToGraph(graph2p1,4)\n",
    "//val graph2 = graph3p1\n",
    "graph3p1.edges.count \n",
    "val dt = ((System.nanoTime-t0)/1.0e6.round/1.0e3).toString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using scala here, we're just going to output the graph as a webpage that we will visualize in a separate page.  The rest of the code here is just reformatting and writing an html file that uses the d3 library.  After this has been run, go to the `site` directory and type:\n",
    "\n",
    "`(py35) python -m http.server`\n",
    "\n",
    "and go to `localhost:8000` (or `remotehost:8000`) to see your graph!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputVertexList = MapPartitionsRDD[3919] at distinct at <console>:77\n",
       "outputVertexListZipped = Array((Terabyte,0), (Google_DeepMind,1), (Watson,2), (List_of_artificial_intelligence_projects,3), (IBM,4), (Brad_Rutter,5), (TOP500,6), (Thomas_Watson,_Jr.,7), (Ginni_Rometty,8), (Artificial_intelligence,9), (Thomas_J._Watson,10), (Timeline_of_electrical_and_electronic_engineering,11), (Computer_performance_by_orders_of_magnitude,12), (Hawthorne,_New_York,13), (Actavis,14), (Wii_U,15), (Watson_(computer),16), (Apple_Inc.,17), (Outline_of_natural_language_processing,18), (Deep_learning,19), (Deep_Blue_(chess_computer),20), (List_of_Jeopardy!_tournaments_and_events,21), (Molecular_Structure_of_Nucleic_Acids:_A_Structure_for_Deoxyribose_Nuc...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((Terabyte,0), (Google_DeepMind,1), (Watson,2), (List_of_artificial_intelligence_projects,3), (IBM,4), (Brad_Rutter,5), (TOP500,6), (Thomas_Watson,_Jr.,7), (Ginni_Rometty,8), (Artificial_intelligence,9), (Thomas_J._Watson,10), (Timeline_of_electrical_and_electronic_engineering,11), (Computer_performance_by_orders_of_magnitude,12), (Hawthorne,_New_York,13), (Actavis,14), (Wii_U,15), (Watson_(computer),16), (Apple_Inc.,17), (Outline_of_natural_language_processing,18), (Deep_learning,19), (Deep_Blue_(chess_computer),20), (List_of_Jeopardy!_tournaments_and_events,21), (Molecular_Structure_of_Nucleic_Acids:_A_Structure_for_Deoxyribose_Nuc..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputVertexList = (graph3p1.triplets.map(x=>x.srcAttr._1) ++ graph3p1.triplets.map(x=>x.dstAttr._1)).distinct\n",
    "\n",
    "// getting unique indices for vertices that will be referenced in the 'links' section of the json output.\n",
    "val outputVertexListZipped=outputVertexList.collect.zipWithIndex\n",
    "\n",
    "def getLinkIndex(name: String): Int = { outputVertexListZipped.filter(x=>x._1==name)(0)._2}\n",
    "centerVertex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirname = site\n",
       "pw = java.io.PrintWriter@26ac3a2c\n",
       "maxEdgeVal = 1173\n",
       "jsonVertices = Array(\"    {\"name\":\"Terabyte\",\"group\":1}\", \"    {\"name\":\"Google_DeepMind\",\"group\":1}\", \"    {\"name\":\"Watson\",\"group\":1}\", \"    {\"name\":\"List_of_artificial_intelligence_projects\",\"group\":1}\", \"    {\"name\":\"IBM\",\"group\":1}\", \"    {\"name\":\"Brad_Rutter\",\"group\":1}\", \"    {\"name\":\"TOP500\",\"group\":1}\", \"    {\"name\":\"Thomas_Watson,_Jr.\",\"group\":1}\", \"    {\"name\":\"Ginni_Rometty\",\"group\":1}\", \"    {\"name\":\"Artificial_intelligence\",\"group\":1}\", \"    {\"name\":\"Thomas_J._Watson\",\"group\":1}\", \"    {\"name\":\"Timeline_of_electrical_and_electronic_engineering\",\"group\":1}\", \"    {\"name\":\"Computer_performance_by_orders_of_magnitude\",\"group\"...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(\"    {\"name\":\"Terabyte\",\"group\":1}\", \"    {\"name\":\"Google_DeepMind\",\"group\":1}\", \"    {\"name\":\"Watson\",\"group\":1}\", \"    {\"name\":\"List_of_artificial_intelligence_projects\",\"group\":1}\", \"    {\"name\":\"IBM\",\"group\":1}\", \"    {\"name\":\"Brad_Rutter\",\"group\":1}\", \"    {\"name\":\"TOP500\",\"group\":1}\", \"    {\"name\":\"Thomas_Watson,_Jr.\",\"group\":1}\", \"    {\"name\":\"Ginni_Rometty\",\"group\":1}\", \"    {\"name\":\"Artificial_intelligence\",\"group\":1}\", \"    {\"name\":\"Thomas_J._Watson\",\"group\":1}\", \"    {\"name\":\"Timeline_of_electrical_and_electronic_engineering\",\"group\":1}\", \"    {\"name\":\"Computer_performance_by_orders_of_magnitude\",\"group\"..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// JSON WRITING ===========================================\n",
    "import sys.process._\n",
    "import java.io._\n",
    "\n",
    "val dirname = \"site\"\n",
    "val pw = new PrintWriter(new File(dirname + \"/\" + centerVertex + \".json\"))\n",
    "\n",
    "// a quick way to normalize edges (not rigorous):\n",
    "val maxEdgeVal = graph3p1.triplets.sortBy(_.attr.toInt, ascending=false).\n",
    "                                        map(x=>x.attr.toInt).take(1)(0)\n",
    "\n",
    "// recall that only *edges* are filtered, the full node list is kept at all times. \n",
    "// (it saves time when rebuilding Graphs)\n",
    "       \n",
    "//formatting vertices (lots of delimiter issues)                        \n",
    "val jsonVertices = outputVertexList.map(x=>x.replace(\"\"\"\\\"\"\" ,\"\"\"\\\\\"\"\")). // backslash\n",
    "                                    map(x=>x.replace(\"\\\"\",\"\\\\\\\"\")). // quote delimiters\n",
    "                                    map(x=>\"    {\\\"name\\\":\\\"\" + x + \"\\\",\\\"group\\\":1}\").\n",
    "                                    collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonEdges = Array(\"    {\"source\":3,\"target\":16,\"value\":11}\", \"    {\"source\":40,\"target\":16,\"value\":9}\", \"    {\"source\":28,\"target\":23,\"value\":2}\", \"    {\"source\":2,\"target\":31,\"value\":6}\", \"    {\"source\":2,\"target\":43,\"value\":5}\", \"    {\"source\":2,\"target\":23,\"value\":2}\", \"    {\"source\":2,\"target\":22,\"value\":1}\", \"    {\"source\":2,\"target\":42,\"value\":9}\", \"    {\"source\":2,\"target\":36,\"value\":3}\", \"    {\"source\":23,\"target\":10,\"value\":4}\", \"    {\"source\":23,\"target\":39,\"value\":4}\", \"    {\"source\":23,\"target\":28,\"value\":3}\", \"    {\"source\":23,\"target\":25,\"value\":6}\", \"    {\"source\":23,\"target\":29,\"value\":2}\", \"    {\"source\":23,\"target\":6,\"value\":2}\", \"    {\"source\":23,\"target\":16,\"value\":5}\", \"    {\"source\":41,\"target\":16,\"value\":6}\", \"    {\"source\":33,\"target\":16,\"value\":12...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(\"    {\"source\":3,\"target\":16,\"value\":11}\", \"    {\"source\":40,\"target\":16,\"value\":9}\", \"    {\"source\":28,\"target\":23,\"value\":2}\", \"    {\"source\":2,\"target\":31,\"value\":6}\", \"    {\"source\":2,\"target\":43,\"value\":5}\", \"    {\"source\":2,\"target\":23,\"value\":2}\", \"    {\"source\":2,\"target\":22,\"value\":1}\", \"    {\"source\":2,\"target\":42,\"value\":9}\", \"    {\"source\":2,\"target\":36,\"value\":3}\", \"    {\"source\":23,\"target\":10,\"value\":4}\", \"    {\"source\":23,\"target\":39,\"value\":4}\", \"    {\"source\":23,\"target\":28,\"value\":3}\", \"    {\"source\":23,\"target\":25,\"value\":6}\", \"    {\"source\":23,\"target\":29,\"value\":2}\", \"    {\"source\":23,\"target\":6,\"value\":2}\", \"    {\"source\":23,\"target\":16,\"value\":5}\", \"    {\"source\":41,\"target\":16,\"value\":6}\", \"    {\"source\":33,\"target\":16,\"value\":12..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonEdges = graph3p1.triplets.map(x => (x.srcAttr._1,x.dstAttr._1,x.attr)).collect.map(y =>  \n",
    "                                    \"    {\\\"source\\\":\" + getLinkIndex(y._1).toString +\n",
    "                                    \",\\\"target\\\":\" + getLinkIndex(y._2).toString +\n",
    "                                    \",\\\"value\\\":\" + (y._3.toFloat/maxEdgeVal*100).ceil.toInt.\n",
    "                                    toString  + \"}\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "// writing json\n",
    "\n",
    "pw.write(\"{\\n\")                             // main header\n",
    "pw.write(\"  \\\"nodes\\\":[\\n\")                 // nodes header\n",
    "\n",
    "for (i<-0 until jsonVertices.length){       // nodes\n",
    "    if(i == jsonVertices.length-1){pw.write(jsonVertices(i))}\n",
    "    else {pw.write(jsonVertices(i)+\",\")}\n",
    "    pw.write(\"\\n\")}\n",
    "pw.write(\"  ],\\n\")                          // end nodes header\n",
    "pw.write(\"  \\\"links\\\":[\\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i<-0 until jsonEdges.length){          // links\n",
    "    if(i == jsonEdges.length-1){pw.write(jsonEdges(i))}\n",
    "    else {pw.write(jsonEdges(i)+\",\")}\n",
    "    pw.write(\"\\n\")\n",
    "}\n",
    "pw.write(\"  ]\\n\") \n",
    "pw.write(\"}\")                               // main footer\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines = Array(<!DOCTYPE html>, <meta charset=\"utf-8\">, <style>, \"\", .node {, \"  stroke: #fff;\", \"  stroke-width: 1.5px;\", }, \"\", .link {, \"  stroke: #999;\", \"  stroke-opacity: .6;\", }, \"\", </style>, <body>, <title>NAME_OF_SITE</title>, \"\", <h1>NAME_OF_SITE </h1>, \"<h3>TIME_FOR_QUERY </h3> \", <script type=\"text/javascript\" src=\"http://mbostock.github.com/d3/d3.js?2.6.0\"></script>, <script type=\"text/javascript\" src=\"http://mbostock.github.com/d3/d3.layout.js?2.6.0\"></script>, <script type=\"text/javascript\" src=\"http://mbostock.github.com/d3/d3.geom.js?2.6.0\"></script>, <script src=\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\"></script>, <script type=\"text/javascript\" charset=\"utf-8\">, \"\", \"    \", var width = 1920,, \"    height = 1000;...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(<!DOCTYPE html>, <meta charset=\"utf-8\">, <style>, \"\", .node {, \"  stroke: #fff;\", \"  stroke-width: 1.5px;\", }, \"\", .link {, \"  stroke: #999;\", \"  stroke-opacity: .6;\", }, \"\", </style>, <body>, <title>NAME_OF_SITE</title>, \"\", <h1>NAME_OF_SITE </h1>, \"<h3>TIME_FOR_QUERY </h3> \", <script type=\"text/javascript\" src=\"http://mbostock.github.com/d3/d3.js?2.6.0\"></script>, <script type=\"text/javascript\" src=\"http://mbostock.github.com/d3/d3.layout.js?2.6.0\"></script>, <script type=\"text/javascript\" src=\"http://mbostock.github.com/d3/d3.geom.js?2.6.0\"></script>, <script src=\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\"></script>, <script type=\"text/javascript\" charset=\"utf-8\">, \"\", \"    \", var width = 1920,, \"    height = 1000;..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// writing new html file from template\n",
    "import scala.io.Source._\n",
    "val lines = fromFile(dirname +\"/template.html\").getLines.toArray\n",
    "val pw = new java.io.PrintWriter(new File(dirname+\"/\"+centerVertex+\".html\"))\n",
    "\n",
    "//lines.map(x=>x.replace(\"NAME_OF_SITE\", centerVertex)).foreach(y=>pw.write(y+\"\\n\"))\n",
    "lines.map(x => x.replace(\"NAME_OF_SITE\", centerVertex)). \n",
    "      map(x => x.replace(\"TIME_FOR_QUERY\", \" (took \" + dt + \"s)\")).\n",
    "      foreach(y=>pw.write(y+\"\\n\"))\n",
    "\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
