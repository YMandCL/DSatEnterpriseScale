{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://ibm.box.com/shared/static/9gegpsmnsoo25ikkbl4qzlvlyjbgxs5x.png\" width = 400> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\"> Spark Fundamentals 1 - Introduction to Spark</h1>\n",
    "<h2 align = \"center\"> Lab 4. Scala - Working with Scala Libraries</h2>\n",
    "<br align = \"left\">\n",
    "\n",
    "**Related free online courses:**\n",
    "\n",
    "Related courses can be found in the following learning paths:\n",
    "\n",
    "- [Spark Fundamentals path](https://cognitiveclass.ai/learn/spark/)\n",
    "- [Big Data Fundamentals path](https://cognitiveclass.ai/learn/big-data/) \n",
    "\n",
    "<img src = \"http://spark.apache.org/images/spark-logo.png\", height = 100, align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark application using Spark SQL\n",
    "\n",
    "Spark SQL provides the ability to write relational queries to be run on Spark. There is the abstraction SchemaRDD which is to create an RDD in which you can run SQL, HiveQL, and Scala. In this lab section, you will use SQL to find out the average weather and precipitation for a given time period in New York. The purpose is to demonstrate how to use the Spark SQL libraries on Spark.\n",
    "\n",
    "### Please note that in Spark 1.3 DataFrames have replaced schemaRDDs however, it is still possible to switch between the two for supporting legacy systems. DataFrames is the recommended method going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first download the data that we will be working with in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// download module to run shell commands within this notebook\n",
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded!\n"
     ]
    }
   ],
   "source": [
    "// download data from IBM Servier\n",
    "// this may take ~30 seconds depending on your internet speed\n",
    "\"wget --quiet https://ibm.box.com/shared/static/j8skrriqeqw66f51iyz911zyqai64j2g.zip\" !\n",
    "\n",
    "println(\"Data Downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted!\n"
     ]
    }
   ],
   "source": [
    "// unzip the folder's content into \"resources\" directory\n",
    "\"unzip -q -o -d ./resources j8skrriqeqw66f51iyz911zyqai64j2g.zip\" !\n",
    "\n",
    "println(\"Data Extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "derby.log\n",
      "followers.txt\n",
      "metastore_db\n",
      "notebook.log\n",
      "nyctaxi.csv\n",
      "nyctaxi100.csv\n",
      "nyctaxisub.csv\n",
      "nycweather.csv\n",
      "pom.xml\n",
      "taxistreams.py\n",
      "users.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// list the extracted files\n",
    "\"ls -1 ./resources/LabData/\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the nycweather data. So run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2013-01-01\",1,0\n",
      "\"2013-01-02\",-2,0\n",
      "\"2013-01-03\",-2,0\n",
      "\"2013-01-04\",1,0\n",
      "\"2013-01-05\",3,0\n",
      "\"2013-01-06\",4,0\n",
      "\"2013-01-07\",5,0\n",
      "\"2013-01-08\",6,0\n",
      "\"2013-01-09\",7,0\n",
      "\"2013-01-10\",7,0\n",
      "\"2013-01-11\",6,13.97\n",
      "\"2013-01-12\",7,0.51\n",
      "\"2013-01-13\",8,0\n",
      "\"2013-01-14\",8,2.29\n",
      "\"2013-01-15\",3,3.05\n",
      "\"2013-01-16\",2,17.53\n",
      "\"2013-01-17\",4,0\n",
      "\"2013-01-18\",-1,0\n",
      "\"2013-01-19\",5,0\n",
      "\"2013-01-20\",6,0\n",
      "\"2013-01-21\",-2,0\n",
      "\"2013-01-22\",-7,0\n",
      "\"2013-01-23\",-9,0\n",
      "\"2013-01-24\",-8,0\n",
      "\"2013-01-25\",-7,1.78\n",
      "\"2013-01-26\",-6,0\n",
      "\"2013-01-27\",-3,0\n",
      "\"2013-01-28\",1,5.59\n",
      "\"2013-01-29\",6,1.52\n",
      "\"2013-01-30\",9,1.02\n",
      "\"2013-01-31\",8,22.86\n",
      "\"2013-02-01\",-2,0\n",
      "\"2013-02-02\",-4,0.51\n",
      "\"2013-02-03\",-3,0.51\n",
      "\"2013-02-04\",-3,0\n",
      "\"2013-02-05\",-1,0.51\n",
      "\"2013-02-06\",1,0\n",
      "\"2013-02-07\",-2,0\n",
      "\"2013-02-08\",-1,29.21\n",
      "\"2013-02-09\",-3,9.65\n",
      "\"2013-02-10\",-3,0\n",
      "\"2013-02-11\",4,12.45\n",
      "\"2013-02-12\",4,0\n",
      "\"2013-02-13\",4,0.76\n",
      "\"2013-02-14\",4,0\n",
      "\"2013-02-15\",8,0\n",
      "\"2013-02-16\",2,0.51\n",
      "\"2013-02-17\",-4,0\n",
      "\"2013-02-18\",-3,0\n",
      "\"2013-02-19\",5,3.81\n",
      "\"2013-02-20\",0,0\n",
      "\"2013-02-21\",-2,0\n",
      "\"2013-02-22\",0,0\n",
      "\"2013-02-23\",4,6.6\n",
      "\"2013-02-24\",5,0.25\n",
      "\"2013-02-25\",4,0\n",
      "\"2013-02-26\",4,3.56\n",
      "\"2013-02-27\",6,39.62\n",
      "\"2013-02-28\",7,0\n",
      "\"2013-03-01\",5,0\n",
      "\"2013-03-02\",2,0\n",
      "\"2013-03-03\",2,0\n",
      "\"2013-03-04\",2,0\n",
      "\"2013-03-05\",4,0\n",
      "\"2013-03-06\",4,0\n",
      "\"2013-03-07\",2,4.83\n",
      "\"2013-03-08\",3,14.22\n",
      "\"2013-03-09\",7,0\n",
      "\"2013-03-10\",6,0\n",
      "\"2013-03-11\",8,0\n",
      "\"2013-03-12\",10,20.07\n",
      "\"2013-03-13\",7,0\n",
      "\"2013-03-14\",2,0\n",
      "\"2013-03-15\",4,0\n",
      "\"2013-03-16\",3,3.05\n",
      "\"2013-03-17\",1,0\n",
      "\"2013-03-18\",0,15.24\n",
      "\"2013-03-19\",3,9.14\n",
      "\"2013-03-20\",4,0\n",
      "\"2013-03-21\",2,0\n",
      "\"2013-03-22\",2,0\n",
      "\"2013-03-23\",4,0\n",
      "\"2013-03-24\",4,0\n",
      "\"2013-03-25\",3,4.32\n",
      "\"2013-03-26\",7,0\n",
      "\"2013-03-27\",7,0\n",
      "\"2013-03-28\",7,0\n",
      "\"2013-03-29\",9,0\n",
      "\"2013-03-30\",10,0\n",
      "\"2013-03-31\",9,2.03\n",
      "\"2013-04-01\",10,0\n",
      "\"2013-04-02\",3,0\n",
      "\"2013-04-03\",4,0\n",
      "\"2013-04-04\",6,0\n",
      "\"2013-04-05\",12,0\n",
      "\"2013-04-06\",7,0\n",
      "\"2013-04-07\",9,0\n",
      "\"2013-04-08\",17,0\n",
      "\"2013-04-09\",19,0\n",
      "\"2013-04-10\",18,12.45\n",
      "\"2013-04-11\",12,0\n",
      "\"2013-04-12\",7,16\n",
      "\"2013-04-13\",10,0.25\n",
      "\"2013-04-14\",11,0\n",
      "\"2013-04-15\",11,0\n",
      "\"2013-04-16\",13,0\n",
      "\"2013-04-17\",17,0.51\n",
      "\"2013-04-18\",13,0.25\n",
      "\"2013-04-19\",17,1.27\n",
      "\"2013-04-20\",11,1.52\n",
      "\"2013-04-21\",8,0\n",
      "\"2013-04-22\",9,0\n",
      "\"2013-04-23\",8,0\n",
      "\"2013-04-24\",14,0\n",
      "\"2013-04-25\",13,0\n",
      "\"2013-04-26\",15,0\n",
      "\"2013-04-27\",16,0\n",
      "\"2013-04-28\",16,0\n",
      "\"2013-04-29\",13,1.02\n",
      "\"2013-04-30\",16,0\n",
      "\"2013-05-01\",14,0\n",
      "\"2013-05-02\",16,0\n",
      "\"2013-05-03\",14,0\n",
      "\"2013-05-04\",15,0\n",
      "\"2013-05-05\",13,0\n",
      "\"2013-05-06\",14,0\n",
      "\"2013-05-07\",17,0\n",
      "\"2013-05-08\",15,76.71\n",
      "\"2013-05-09\",16,12.7\n",
      "\"2013-05-10\",21,0.25\n",
      "\"2013-05-11\",19,27.69\n",
      "\"2013-05-12\",16,0\n",
      "\"2013-05-13\",11,0\n",
      "\"2013-05-14\",11,0\n",
      "\"2013-05-15\",17,0\n",
      "\"2013-05-16\",22,0\n",
      "\"2013-05-17\",18,0\n",
      "\"2013-05-18\",16,0.25\n",
      "\"2013-05-19\",14,15.24\n",
      "\"2013-05-20\",21,0\n",
      "\"2013-05-21\",25,0\n",
      "\"2013-05-22\",21,0\n",
      "\"2013-05-23\",22,45.97\n",
      "\"2013-05-24\",13,7.62\n",
      "\"2013-05-25\",10,3.56\n",
      "\"2013-05-26\",14,0\n",
      "\"2013-05-27\",17,0\n",
      "\"2013-05-28\",17,13.21\n",
      "\"2013-05-29\",21,0\n",
      "\"2013-05-30\",27,0\n",
      "\"2013-05-31\",28,0\n",
      "\"2013-06-01\",28,0\n",
      "\"2013-06-02\",26,21.59\n",
      "\"2013-06-03\",22,22.1\n",
      "\"2013-06-04\",19,0\n",
      "\"2013-06-05\",19,0\n",
      "\"2013-06-06\",18,3.3\n",
      "\"2013-06-07\",16,105.66\n",
      "\"2013-06-08\",19,12.19\n",
      "\"2013-06-09\",22,0\n",
      "\"2013-06-10\",19,35.05\n",
      "\"2013-06-11\",22,2.29\n",
      "\"2013-06-12\",22,0\n",
      "\"2013-06-13\",17,32\n",
      "\"2013-06-14\",17,9.65\n",
      "\"2013-06-15\",22,0\n",
      "\"2013-06-16\",23,0\n",
      "\"2013-06-17\",25,0.25\n",
      "\"2013-06-18\",23,4.83\n",
      "\"2013-06-19\",20,0.25\n",
      "\"2013-06-20\",22,0\n",
      "\"2013-06-21\",23,0\n",
      "\"2013-06-22\",24,0\n",
      "\"2013-06-23\",26,0\n",
      "\"2013-06-24\",28,0\n",
      "\"2013-06-25\",28,0\n",
      "\"2013-06-26\",27,1.27\n",
      "\"2013-06-27\",27,6.1\n",
      "\"2013-06-28\",26,0\n",
      "\"2013-06-29\",25,0\n",
      "\"2013-06-30\",27,0\n",
      "\"2013-07-01\",24,21.34\n",
      "\"2013-07-02\",25,2.03\n",
      "\"2013-07-03\",26,13.46\n",
      "\"2013-07-04\",27,0\n",
      "\"2013-07-05\",28,0\n",
      "\"2013-07-06\",29,0\n",
      "\"2013-07-07\",29,0\n",
      "\"2013-07-08\",27,5.59\n",
      "\"2013-07-09\",27,5.84\n",
      "\"2013-07-10\",27,0\n",
      "\"2013-07-11\",27,0\n",
      "\"2013-07-12\",23,6.35\n",
      "\"2013-07-13\",23,1.52\n",
      "\"2013-07-14\",28,0\n",
      "\"2013-07-15\",30,0\n",
      "\"2013-07-16\",30,0\n",
      "\"2013-07-17\",31,0\n",
      "\"2013-07-18\",32,0\n",
      "\"2013-07-19\",32,0\n",
      "\"2013-07-20\",31,0\n",
      "\"2013-07-21\",28,0\n",
      "\"2013-07-22\",27,1.52\n",
      "\"2013-07-23\",27,7.87\n",
      "\"2013-07-24\",24,0\n",
      "\"2013-07-25\",19,0.25\n",
      "\"2013-07-26\",23,0\n",
      "\"2013-07-27\",24,0\n",
      "\"2013-07-28\",23,6.1\n",
      "\"2013-07-29\",25,0.25\n",
      "\"2013-07-30\",24,0\n",
      "\"2013-07-31\",24,0\n",
      "\"2013-08-01\",22,16.51\n",
      "\"2013-08-02\",24,0\n",
      "\"2013-08-03\",23,1.52\n",
      "\"2013-08-04\",23,0\n",
      "\"2013-08-05\",21,0\n",
      "\"2013-08-06\",23,0\n",
      "\"2013-08-07\",24,0\n",
      "\"2013-08-08\",24,11.68\n",
      "\"2013-08-09\",27,1.27\n",
      "\"2013-08-10\",25,0\n",
      "\"2013-08-11\",23,0\n",
      "\"2013-08-12\",24,1.27\n",
      "\"2013-08-13\",23,21.59\n",
      "\"2013-08-14\",20,0\n",
      "\"2013-08-15\",21,0\n",
      "\"2013-08-16\",23,0\n",
      "\"2013-08-17\",23,0\n",
      "\"2013-08-18\",22,0\n",
      "\"2013-08-19\",23,0\n",
      "\"2013-08-20\",26,0\n",
      "\"2013-08-21\",27,0\n",
      "\"2013-08-22\",24,6.35\n",
      "\"2013-08-23\",25,0\n",
      "\"2013-08-24\",23,0\n",
      "\"2013-08-25\",23,0\n",
      "\"2013-08-26\",24,1.02\n",
      "\"2013-08-27\",26,0.25\n",
      "\"2013-08-28\",26,10.92\n",
      "\"2013-08-29\",24,0\n",
      "\"2013-08-30\",26,0\n",
      "\"2013-08-31\",27,0\n",
      "\"2013-09-01\",27,0\n",
      "\"2013-09-02\",26,1.27\n",
      "\"2013-09-03\",24,0.76\n",
      "\"2013-09-04\",23,0\n",
      "\"2013-09-05\",22,0\n",
      "\"2013-09-06\",18,0\n",
      "\"2013-09-07\",21,0\n",
      "\"2013-09-08\",23,0\n",
      "\"2013-09-09\",18,0\n",
      "\"2013-09-10\",26,0.25\n",
      "\"2013-09-11\",31,0\n",
      "\"2013-09-12\",26,40.64\n",
      "\"2013-09-13\",20,1.52\n",
      "\"2013-09-14\",16,0\n",
      "\"2013-09-15\",17,0\n",
      "\"2013-09-16\",18,0.76\n",
      "\"2013-09-17\",14,0\n",
      "\"2013-09-18\",17,0\n",
      "\"2013-09-19\",19,0\n",
      "\"2013-09-20\",21,0\n",
      "\"2013-09-21\",21,18.29\n",
      "\"2013-09-22\",17,11.43\n",
      "\"2013-09-23\",14,0\n",
      "\"2013-09-24\",16,0\n",
      "\"2013-09-25\",17,0\n",
      "\"2013-09-26\",18,0\n",
      "\"2013-09-27\",17,0\n",
      "\"2013-09-28\",18,0\n",
      "\"2013-09-29\",18,0\n",
      "\"2013-09-30\",19,0\n",
      "\"2013-10-01\",22,0\n",
      "\"2013-10-02\",23,0\n",
      "\"2013-10-03\",22,0\n",
      "\"2013-10-04\",24,0\n",
      "\"2013-10-05\",21,0\n",
      "\"2013-10-06\",20,0\n",
      "\"2013-10-07\",20,6.35\n",
      "\"2013-10-08\",16,0\n",
      "\"2013-10-09\",14,0\n",
      "\"2013-10-10\",16,0\n",
      "\"2013-10-11\",18,0.51\n",
      "\"2013-10-12\",19,0\n",
      "\"2013-10-13\",16,0\n",
      "\"2013-10-14\",15,0\n",
      "\"2013-10-15\",17,0\n",
      "\"2013-10-16\",17,0\n",
      "\"2013-10-17\",19,0.51\n",
      "\"2013-10-18\",17,0\n",
      "\"2013-10-19\",14,0.25\n",
      "\"2013-10-20\",14,0\n",
      "\"2013-10-21\",14,0\n",
      "\"2013-10-22\",15,0\n",
      "\"2013-10-23\",10,0\n",
      "\"2013-10-24\",9,0\n",
      "\"2013-10-25\",8,0\n",
      "\"2013-10-26\",9,0\n",
      "\"2013-10-27\",11,0\n",
      "\"2013-10-28\",11,0\n",
      "\"2013-10-29\",10,0\n",
      "\"2013-10-30\",12,0\n",
      "\"2013-10-31\",16,1.52\n",
      "\"2013-11-01\",18,3.3\n",
      "\"2013-11-02\",17,0\n",
      "\"2013-11-03\",8,0\n",
      "\"2013-11-04\",5,0\n",
      "\"2013-11-05\",9,0\n",
      "\"2013-11-06\",13,0\n",
      "\"2013-11-07\",12,3.3\n",
      "\"2013-11-08\",7,0\n",
      "\"2013-11-09\",7,0\n",
      "\"2013-11-10\",12,0\n",
      "\"2013-11-11\",9,0\n",
      "\"2013-11-12\",6,0.76\n",
      "\"2013-11-13\",1,0\n",
      "\"2013-11-14\",6,0\n",
      "\"2013-11-15\",11,0\n",
      "\"2013-11-16\",12,1.27\n",
      "\"2013-11-17\",13,0.76\n",
      "\"2013-11-18\",14,5.59\n",
      "\"2013-11-19\",7,0\n",
      "\"2013-11-20\",3,0\n",
      "\"2013-11-21\",7,0\n",
      "\"2013-11-22\",12,1.78\n",
      "\"2013-11-23\",6,0\n",
      "\"2013-11-24\",-3,0\n",
      "\"2013-11-25\",-2,0\n",
      "\"2013-11-26\",4,12.95\n",
      "\"2013-11-27\",9,50.29\n",
      "\"2013-11-28\",1,0\n",
      "\"2013-11-29\",1,0\n",
      "\"2013-11-30\",0,0\n",
      "\"2013-12-01\",6,0\n",
      "\"2013-12-02\",7,0\n",
      "\"2013-12-03\",8,0\n",
      "\"2013-12-04\",8,0\n",
      "\"2013-12-05\",12,0.25\n",
      "\"2013-12-06\",10,18.54\n",
      "\"2013-12-07\",3,3.56\n",
      "\"2013-12-08\",-1,2.03\n",
      "\"2013-12-09\",2,7.62\n",
      "\"2013-12-10\",1,5.84\n",
      "\"2013-12-11\",-1,0\n",
      "\"2013-12-12\",-3,0\n",
      "\"2013-12-13\",-2,0\n",
      "\"2013-12-14\",-2,18.54\n",
      "\"2013-12-15\",2,18.29\n",
      "\"2013-12-16\",-2,0\n",
      "\"2013-12-17\",-2,4.83\n",
      "\"2013-12-18\",-1,0\n",
      "\"2013-12-19\",4,0\n",
      "\"2013-12-20\",8,0\n",
      "\"2013-12-21\",14,0.25\n",
      "\"2013-12-22\",19,0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lines = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\"2013-01-01\",1,0\n",
       "\"2013-01-02\",-2,0\n",
       "\"2013-01-03\",-2,0\n",
       "\"2013-01-04\",1,0\n",
       "\"2013-01-05\",3,0\n",
       "\"2013-01-06\",4,0\n",
       "\"2013-01-07\",5,0\n",
       "\"2013-01-08\",6,0\n",
       "\"2013-01-09\",7,0\n",
       "\"2013-01-10\",7,0\n",
       "\"2013-01-11\",6,13.97\n",
       "\"2013-01-12\",7,0.51\n",
       "\"2013-01-13\",8,0\n",
       "\"2013-01-14\",8,2.29\n",
       "\"2013-01-15\",3,3.05\n",
       "\"2013-01-16\",2,17.53\n",
       "\"2013-01-17\",4,0\n",
       "\"2013-01-18\",-1,0\n",
       "\"2013-01-19\",5,0\n",
       "\"2013-01-20\",6,0\n",
       "\"2013-01-21\",-2,0\n",
       "\"2013-01-22\",-7,0\n",
       "\"2013-01-23\",-9,0\n",
       "\"2013-01-24\",-8,0\n",
       "\"2013-01-25\",-7,1.78\n",
       "\"2013-01-26\",-6,0\n",
       "\"2013-01-27\",-3,0\n",
       "\"2013-01-28\",1,5.59\n",
       "\"2013-01-29\",6,1.52\n",
       "\"2013-01-30\",9,1.02\n",
       "\"2013-01-31\",8,22.86\n",
       "\"2013-02-01\",-2,0\n",
       "\"2013-02-02\",-4,0.51\n",
       "\"2013-02-03\",-3,0.51\n",
       "\"2013-02-04\",-3,0\n",
       "\"2013-02-05\",-1,0.51\n",
       "\"2013-02-06\",1,0\n",
       "\"2013-02-07\",-2,0\n",
       "\"2013-02-08\",-1,29.21\n",
       "\"2013-02-09\",-3,9.65\n",
       "\"2013-02-10\",-3,0\n",
       "\"2013-02-11\",4,...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val lines = scala.io.Source.fromFile(\"./resources/LabData/nycweather.csv\").mkString\n",
    "println(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three columns in the dataset, the date, the mean temperature in Celsius, and the precipitation for the day. Since we already know the schema, we will infer the schema using reflection.\n",
    "\n",
    "You will first need to define the SparkSQL context. Do so by creating it from an existing SparkContext. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@4f53832\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@4f53832"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to import a library for creating a SchemaRDD. Type this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a case class in Scala that defines the schema of the table. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Weather\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Weather(date: String, temp: Int, precipitation: Double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the RDD of the Weather object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather = [date: string, temp: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, temp: int ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val weather = sc.textFile(\"/resources/LabData/nycweather.csv\").map(_.split(\",\")). map(w => Weather(w(0), w(1).trim.toInt, w(2).trim.toDouble)).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first load in the file, and then you map it by splitting it up by the commas and then another mapping to get it into the Weather class.\n",
    "\n",
    "Next you need to register the RDD as a table. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weather.registerTempTable(\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you are ready to create and run some queries on the RDD. You want to get a list of the hottest dates with some precipitation. Type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.catalyst.errors.package$TreeNodeException\n",
       "Message: execute, tree:\n",
       "Exchange rangepartitioning(temp#5 DESC NULLS LAST, 200)\n",
       "+- *(1) Filter (precipitation#6 > 0.0)\n",
       "   +- *(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, $line40.$read$$iw$$iw$Weather, true]).date, true, false) AS date#4, assertnotnull(input[0, $line40.$read$$iw$$iw$Weather, true]).temp AS temp#5, assertnotnull(input[0, $line40.$read$$iw$$iw$Weather, true]).precipitation AS precipitation#6]\n",
       "      +- Scan ExternalRDDScan[obj#3]\n",
       "\n",
       "StackTrace: Exchange rangepartitioning(temp#5 DESC NULLS LAST, 200)\n",
       "+- *(1) Filter (precipitation#6 > 0.0)\n",
       "   +- *(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, Weather, true]).date, true, false) AS date#4, assertnotnull(input[0, Weather, true]).temp AS temp#5, assertnotnull(input[0, Weather, true]).precipitation AS precipitation#6]\n",
       "      +- Scan ExternalRDDScan[obj#3]\n",
       "  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n",
       "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)\n",
       "  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n",
       "  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:294)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2722)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2722)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n",
       "  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2722)\n",
       "  ... 46 elided\n",
       "Caused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/resources/LabData/nycweather.csv\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\n",
       "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:225)\n",
       "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n",
       "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n",
       "  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n",
       "  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hottest_with_precip = sqlContext.sql(\"SELECT * FROM weather WHERE precipitation > 0.0 ORDER BY temp DESC\")\n",
    "\n",
    "hottest_with_precip.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal RDD operations will work. Print the top hottest days with some precipitation out to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:36: error: not found: value hottest_with_precip\n",
       "       hottest_with_precip.map(x => (\"Date: \" + x(0), \"Temp : \" + x(1), \"Precip: \" + x(2))).top(10).foreach(println)\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_with_precip.map(x => (\"Date: \" + x(0), \"Temp : \" + x(1), \"Precip: \" + x(2))).top(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark application using MLlib\n",
    "\n",
    "In this section, Spark will be used to acquire the K-Means clustering for drop-off latitudes and longitudes of taxis for 3 clusters. The sample data contains a subset of taxi trips with hack license, medallion, pickup date/time, drop off date/time, pickup/drop off latitude/longitude, passenger count, trip distance, trip time and other information. As such, this may give a good indication of where to best to hail a cab.\n",
    "\n",
    "Remember, this is only a subset of the file that you used in a previous exercise. If you ran this exercise on the full dataset, it would take a long time as we are only running on a test environment with limited resources.\n",
    "\n",
    "Import the needed packages for K-Means algorithm and Vector packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxiFile = /resources/LabData/nyctaxisub.csv MapPartitionsRDD[8] at textFile at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/resources/LabData/nyctaxisub.csv MapPartitionsRDD[8] at textFile at <console>:37"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxiFile = sc.textFile(\"/resources/LabData/nyctaxisub.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the number of rows in taxiFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.InvalidInputException\n",
       "Message: Input path does not exist: file:/resources/LabData/nyctaxisub.csv\n",
       "StackTrace:   at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1162)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiFile.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxiData = MapPartitionsRDD[11] at filter at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[11] at filter at <console>:41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxiData=taxiFile.filter(_.contains(\"2013\")).\n",
    "    filter(_.split(\",\")(3)!=\"\" ).    //dropoff_latitude\n",
    "    filter(_.split(\",\")(4)!=\"\")      //dropoff_longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first filter limits the rows to those that occurred in the year 2013. This will also remove any header in the file. The third and fourth columns contain the drop off latitude and longitude. The transformation will throw exceptions if these values are empty.\n",
    "\n",
    "Do another count to see what was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.InvalidInputException\n",
       "Message: Input path does not exist: file:/resources/LabData/nyctaxisub.csv\n",
       "StackTrace:   at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1162)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, if we had used the full set of data, it would have filtered out a great many more lines.\n",
    "\n",
    "To fence the area roughly to New York City use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxiFence = MapPartitionsRDD[15] at filter at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at filter at <console>:45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxiFence=taxiData.\n",
    "    filter(_.split(\",\")(3).toDouble>40.70).\n",
    "    filter(_.split(\",\")(3).toDouble<40.86).\n",
    "    filter(_.split(\",\")(4).toDouble>(-74.02)).\n",
    "    filter(_.split(\",\")(4).toDouble<(-73.93))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine how many are left in taxiFence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.InvalidInputException\n",
       "Message: Input path does not exist: file:/resources/LabData/nyctaxisub.csv\n",
       "StackTrace:   at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1162)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiFence.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately, 43,354 rows were dropped since these drop-off points are outside of New York City.\n",
    "\n",
    "Create Vectors with the latitudes and longitudes that will be used as input to the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxi = MapPartitionsRDD[16] at map at <console>:44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at map at <console>:44"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxi=taxiFence.\n",
    "    map{\n",
    "        line=>Vectors.dense(\n",
    "            line.split(',').slice(3,5).map(_ .toDouble)\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:42: error: not found: value taxi\n",
       "       val model=KMeans.train(taxi,clusterCount,iterationCount)\n",
       "                              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iterationCount=10\n",
    "val clusterCount=3\n",
    "\n",
    "val model=KMeans.train(taxi,clusterCount,iterationCount)\n",
    "val clusterCenters=model.clusterCenters.map(_.toArray)\n",
    "\n",
    "clusterCenters.foreach(lines=>println(lines(0),lines(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the map co-ordinates. Not surprisingly, the second point is between the Theater District and Grand Central. The third point is in The Village, NYU, Soho and Little Italy area. The first point is the Upper East Side, presumably where people are more likely to take cabs than subways.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark application using Spark Streaming\n",
    "\n",
    "This section focuses on Spark Streams, an easy to build, scalable, stateful (e.g. sliding windows) stream processing library. Streaming jobs are written the same way Spark batch jobs are coded and support Java, Scala and Python. In this exercise, taxi trip data will be streamed using a socket connection and then analyzed to provide a summary of number of passengers by taxi vendor. This will be implemented in the Spark shell using Scala.\n",
    "\n",
    "There are two relevant files for this section. The first one is the nyctaxi100.csv which will serve as the source of the stream. The other file is a python file, taxistreams.py, which will feed the csv file through a socket connection to simulate a stream.\n",
    "\n",
    "### <span style=\"color: red\">IN ORDER TO START THE STREAM PLEASE OPEN A NEW PYTHON NOTEBOOK AND RUN THE CODE BELOW IN IT:</span> \n",
    "\n",
    "To open a new Python notebook click on the blue notebook button at the top right of this page, next to the search box. Choose PYTHON 2 and then copy and past the code below into the cell in the new Python notebook. Run the cell as normal. To interrupt the kernel hit the STOP button in the Action buttons above.\n",
    "\n",
    "```\n",
    "!python /resources/LabData/taxistreams.py\n",
    "\n",
    "```\n",
    "\n",
    "Once started, the program will bind and listen to the localhost socket 7777. When a connection is made, it will read ‘nyctaxi100.csv’ and send across the socket. The sleep is set such that one line will be sent every 0.5 seconds, or 2 rows a second. This was intentionally set to a high value to make it easier to view the data during execution.\n",
    "\n",
    "Turn off logging so that you can see the output of the application and Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.StreamingContext._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the StreamingContext by using the existing SparkContext (sc). It will be using a 1 second batch interval, which means the stream is divided to 1 second batches and each batch becomes a RDD. This is intentional to make it easier to read the data during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssc = org.apache.spark.streaming.StreamingContext@10344997\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.StreamingContext@10344997"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ssc = new StreamingContext(sc,Seconds(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the socket stream that connects to the localhost socket 7777. This matches the port that the Python script is listening on. Each batch from the Stream be a lines RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines = org.apache.spark.streaming.dstream.SocketInputDStream@27b2dad9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.dstream.SocketInputDStream@27b2dad9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = ssc.socketTextStream(\"localhost\",7777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, put in the business logic to split up the lines on each comma and mapping pass(15), which is the vendor, and pass(7), which is the passenger count. Then this is reduced by key resulting in a summary of number of passengers by vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pass = org.apache.spark.streaming.dstream.ShuffledDStream@4c8a0136\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.dstream.ShuffledDStream@4c8a0136"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pass = lines.map(_.split(\",\")).\n",
    "    map(pass=>(pass(15),pass(7).toInt)).\n",
    "    reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two line starts the stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1575346559000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346560000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346561000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",5)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346562000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",5)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346563000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346564000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346565000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346566000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",1)\n",
      "(\"VTS\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346567000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346568000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346569000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346570000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",4)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.InterruptedException\n",
       "Message: null\n",
       "StackTrace:   at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)\n",
       "  at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2048)\n",
       "  at org.apache.spark.streaming.ContextWaiter.waitForStopOrError(ContextWaiter.scala:63)\n",
       "  at org.apache.spark.streaming.StreamingContext.awaitTermination(StreamingContext.scala:618)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1575346571000 ms\n",
      "-------------------------------------------\n",
      "(\"VTS\",6)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346572000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",4)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346573000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346574000 ms\n",
      "-------------------------------------------\n",
      "(\"CMT\",2)\n",
      "(\"VTS\",1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346575000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346576000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346577000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346578000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346579000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346580000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346581000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346582000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346583000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346584000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346585000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346586000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346587000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346588000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346589000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346590000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346591000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346592000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346593000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346594000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346595000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346596000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346597000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346598000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346599000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346600000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346601000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346602000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346603000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346604000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346605000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346606000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346607000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346608000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346609000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346610000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346611000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346612000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346613000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346614000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346615000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346616000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346617000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346618000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346619000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346620000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346621000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346622000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346623000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346624000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346625000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346626000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346627000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346628000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346629000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346630000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346631000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346632000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346633000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346634000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346635000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346636000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346637000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346638000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346639000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346640000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346641000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346642000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346643000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 1575346644000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346645000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346646000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346647000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346648000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346649000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346650000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346651000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346652000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346653000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346654000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346655000 ms\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 1575346656000 ms\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take a few cycles for the connection to be recognized, and then the data is sent. In this case, 2 rows per second of taxi trip data is receive in a 1 second batch interval.\n",
    "\n",
    "In the Python terminal, the contents of the file are printed as they are streamed.\n",
    "\n",
    "**Note: TO STOP THE STREAM PLEASE INTERRUPT THE KERNEL IN BOTH THE OTHER PYTHON NOTEBOOK AND THIS NOTEBOOK. THEN RESTART THIS NOTEBOOK'S KERNEL TO CONTINUE ONTO THE GRAPHX APPLICATION**\n",
    "\n",
    "This is just a simple example showing how you can take streaming data into Spark and do some type of processing on it. In the case here, the taxi and the number of passengers was extracted from the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark application using GraphX\n",
    "\n",
    "Users.txt is a set of users and followers is the relationship between the users. Take a look at the contents of these two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: \n",
      "1,BarackObama,Barack Obama\n",
      "2,ladygaga,Goddess of Love\n",
      "3,jeresig,John Resig\n",
      "4,justinbieber,Justin Bieber\n",
      "6,matei_zaharia,Matei Zaharia\n",
      "7,odersky,Martin Odersky\n",
      "8,anonsys\n",
      "\n",
      "Followers: \n",
      "2 1\n",
      "4 1\n",
      "1 2\n",
      "6 3\n",
      "7 3\n",
      "7 6\n",
      "6 7\n",
      "3 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Users: \")\n",
    "println(scala.io.Source.fromFile(\"./resources/LabData/users.txt\").mkString)\n",
    "\n",
    "println(\"Followers: \")\n",
    "println(scala.io.Source.fromFile(\"./resources/LabData/followers.txt\").mkString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the GraphX package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the users RDD and parse into tuples of user id and attribute list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,[Ljava.lang.String;@f264a68)\n",
      "(2,[Ljava.lang.String;@3640ae82)\n",
      "(3,[Ljava.lang.String;@6e7fbcd0)\n",
      "(4,[Ljava.lang.String;@13707999)\n",
      "(6,[Ljava.lang.String;@2e381495)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "users = MapPartitionsRDD[197] at map at <console>:54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[197] at map at <console>:54"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = (sc.textFile(\"./resources/LabData/users.txt\").map(line => line.split(\",\")).map(parts => (parts.head.toLong, parts.tail)))\n",
    "\n",
    "users.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the edge data, which is already in userId -> userId format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "followerGraph = org.apache.spark.graphx.impl.GraphImpl@51cdcea3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@51cdcea3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val followerGraph = GraphLoader.edgeListFile(sc, \"./resources/LabData/followers.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the user attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph = org.apache.spark.graphx.impl.GraphImpl@19e0c5b8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@19e0c5b8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph = followerGraph.outerJoinVertices(users) {\n",
    "    case (uid, deg, Some(attrList)) => attrList\n",
    "    case (uid, deg, None) => Array.empty[String]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the graph to users with usernames and names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subgraph = org.apache.spark.graphx.impl.GraphImpl@185a37ed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@185a37ed"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pagerankGraph = org.apache.spark.graphx.impl.GraphImpl@3850b08e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@3850b08e"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pagerankGraph = subgraph.pageRank(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the attributes of the top pagerank users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userInfoWithPageRank = org.apache.spark.graphx.impl.GraphImpl@1ae21320\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@1ae21320"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {\n",
    "    case (uid, attrList, Some(pr)) => (pr, attrList.toList)\n",
    "    case (uid, attrList, None) => (0.0, attrList.toList)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the line out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,(1.4610558475474507,List(BarackObama, Barack Obama)))\n",
      "(2,(1.3926425103962674,List(ladygaga, Goddess of Love)))\n",
      "(7,(1.2956193310217194,List(odersky, Martin Odersky)))\n",
      "(3,(0.9985540153884633,List(jeresig, John Resig)))\n",
      "(6,(0.7013832556651652,List(matei_zaharia, Matei Zaharia)))\n"
     ]
    }
   ],
   "source": [
    "println(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "**Tip**: Enjoyed using Jupyter notebooks with Spark? Get yourself a free \n",
    "    <a href=\"http://cocl.us/DSX_on_Cloud\">IBM Cloud</a> account where you can use Data Science Experience notebooks\n",
    "    and have *two* Spark executors for free!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Having completed this exercise, you should have some familiarity with using the Spark libraries. In particular, you use Spark SQL to effectively query data inside of Spark. You used Spark Streaming to process incoming streams of batch data. You used Spark's MLlib to compute the *k*-means algorithm to find the best place to hail a cab. Finally, you used Spark's GraphX library to perform and parallel graph calculations on a dataset to find the attributes of the top users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the free course on **Cognitive Class** called *Spark Fundamentals I*. If you accessed this notebook outside the course, you can take this free self-paced course, online by going to: https://cognitiveclass.ai/courses/what-is-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:  \n",
    "Hi! It's [Alex Aklson](https://www.linkedin.com/in/aklson/), one of the authors of this notebook. I hope you found this lab educational! There is much more to learn about Spark but you are well on your way. Feel free to connect with me if you have any questions.\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
