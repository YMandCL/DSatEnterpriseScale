{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Spark using Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30)\n",
       "xrangeRDD = ParallelCollectionRDD[0] at parallelize at <console>:30\n",
       "subRDD = MapPartitionsRDD[1] at map at <console>:31\n",
       "filteredRDD = MapPartitionsRDD[2] at filter at <console>:32\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = 1 to 30\n",
    "\n",
    "// most RDD operations have identical or nearly identical syntax:\n",
    "val xrangeRDD = sc.parallelize(data, 4)\n",
    "val subRDD = xrangeRDD.map(x => x-1)\n",
    "val filteredRDD = subRDD.filter(x => x<10)\n",
    "filteredRDD.collect()\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://rally1.fyre.ibm.com:4044)\" target=\"new_tab\">Spark UI: local-1526450241605</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1526450241605: Some(http://rally1.fyre.ibm.com:4044)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sc.version)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test = ParallelCollectionRDD[3] at parallelize at <console>:27\n",
       "t1 = 454365386432198\n",
       "dt1 = 0.305775544\n",
       "t2 = 454365692225735\n",
       "dt2 = 0.122984662\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.122984662"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = sc.parallelize(1 to 50000,50)\n",
    "//cache this data\n",
    "test.cache\n",
    "\n",
    "val t1 = System.nanoTime()\n",
    "// first count will trigger evaluation of count *and* cache\n",
    "test.count\n",
    "val dt1 = (System.nanoTime() - t1).toDouble/1.0e9\n",
    "\n",
    "val t2 = System.nanoTime()\n",
    "// second count operates on cached data only\n",
    "test.count\n",
    "val dt2 = (System.nanoTime() - t2).toDouble/1.0e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@7189fa57\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://rally1.fyre.ibm.com:4044)\" target=\"new_tab\">Spark UI: local-1526450241605</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1526450241605: Some(http://rally1.fyre.ibm.com:4044)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import spark.implicits._ \n",
    "//used for $ notation\n",
    "\n",
    "val df = spark.read.json(\"people.json\")\n",
    "df.show\n",
    "df.printSchema\n",
    "\n",
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: select statements\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//neeed to register dataframe to use (hive like) sql\n",
    "println(\"Query 1: select statements\")\n",
    "df.select(\"name\").show\n",
    "spark.sql(\"SELECT name FROM people\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: filter statements\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Query 2: filter statements\")\n",
    "df.filter(df(\"age\") > 21).show\n",
    "df.filter($\"age\" > 21).show\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3: group by statements\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    0|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Query 3: group by statements\")\n",
    "df.groupBy(\"age\").count().show\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
