{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Spark using Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Context has been created for you upon initialization of the Toree kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://rally1.fyre.ibm.com:4043)\" target=\"new_tab\">Spark UI: local-1526848175159</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1526848175159: Some(http://rally1.fyre.ibm.com:4043)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sc.version)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we create an RDD here by calling `sc.parallelize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30)\n",
       "xrangeRDD = ParallelCollectionRDD[0] at parallelize at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:28"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = 1 to 30 \n",
    "val xrangeRDD = sc.parallelize(data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Transormations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scala API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subRDD = MapPartitionsRDD[1] at map at <console>:30\n",
       "filteredRDD = MapPartitionsRDD[2] at filter at <console>:31\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at filter at <console>:31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val subRDD = xrangeRDD.map(x => x - 1)\n",
    "val filteredRDD = subRDD.filter(x => x < 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation returns a result to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Count: \" + filteredRDD.count())\n",
    "filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test = ParallelCollectionRDD[3] at parallelize at <console>:27\n",
       "t1 = 852300669254662\n",
       "dt1 = 0.242165945\n",
       "t2 = 852300911432550\n",
       "dt2 = 0.084373291\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.084373291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = sc.parallelize(1 to 50000,50)\n",
    "//cache this data\n",
    "test.cache\n",
    "\n",
    "val t1 = System.nanoTime()\n",
    "// first count will trigger evaluation of count *and* cache\n",
    "test.count\n",
    "val dt1 = (System.nanoTime() - t1).toDouble/1.0e9\n",
    "\n",
    "val t2 = System.nanoTime()\n",
    "\n",
    "\n",
    "test.count\n",
    "val dt2 = (System.nanoTime() - t2).toDouble/1.0e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@546731b4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://rally1.fyre.ibm.com:4045)\" target=\"new_tab\">Spark UI: local-1526852416942</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1526852416942: Some(http://rally1.fyre.ibm.com:4045)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark SQL basic example\")\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"people.json\")\n",
    "df.show\n",
    "df.printSchema\n",
    "\n",
    "// Register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: select statements\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//neeed to register dataframe to use (hive like) sql\n",
    "println(\"Query 1: select statements\")\n",
    "df.select(\"name\").show\n",
    "spark.sql(\"SELECT name FROM people\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: filter statements\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Query 2: filter statements\")\n",
    "df.filter(df(\"age\") > 21).show\n",
    "df.filter($\"age\" > 21).show\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3: group by statements\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    0|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Query 3: group by statements\")\n",
    "df.groupBy(\"age\").count().show\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
